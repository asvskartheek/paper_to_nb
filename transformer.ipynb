{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Is All You Need — Transformer Implementation\n",
    "\n",
    "A faithful implementation of the original Transformer from [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "**Task:** German → English translation on IWSLT 2014 (a scaled-down version of the paper's WMT 2014 benchmark).\n",
    "\n",
    "**Architecture:** Identical to the paper — post-norm residual connections, sinusoidal positional encoding, three-way weight tying, label-smoothed cross-entropy, and the paper's exact LR schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup & Installs\n",
    "!pip install datasets tokenizers sacrebleu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    # Model architecture (scaled down from paper's base config)\n",
    "    n_layers: int = 3           # Paper: 6\n",
    "    d_model: int = 256          # Paper: 512\n",
    "    d_ff: int = 512             # Paper: 2048\n",
    "    n_heads: int = 4            # Paper: 8\n",
    "    d_k: int = 64               # Paper: 64 (same)\n",
    "    dropout: float = 0.1        # Paper: 0.1 (same)\n",
    "    max_seq_len: int = 128\n",
    "    \n",
    "    # Training\n",
    "    label_smoothing: float = 0.1  # Paper: 0.1 (same)\n",
    "    warmup_steps: int = 4000      # Paper: 4000 (same)\n",
    "    n_epochs: int = 25\n",
    "    batch_size: int = 64\n",
    "    \n",
    "    # Tokenizer\n",
    "    vocab_size: int = 10000\n",
    "    \n",
    "    # Adam params (Section 5.3)\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.98\n",
    "    adam_eps: float = 1e-9       # Paper: 10^-9 (not PyTorch default 10^-8)\n",
    "    \n",
    "    # Special token IDs (set after tokenizer training)\n",
    "    pad_idx: int = 0\n",
    "    bos_idx: int = 1\n",
    "    eos_idx: int = 2\n",
    "    unk_idx: int = 3\n",
    "    \n",
    "    seed: int = 42\n",
    "\n",
    "config = TransformerConfig()\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\\nConfig: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "\n",
    "# 1. Download IWSLT14 DE-EN\n",
    "print(\"Downloading IWSLT14 DE-EN...\")\n",
    "raw_dataset = load_dataset(\"bbaaaa/iwslt14-de-en\")\n",
    "print(f\"Train: {len(raw_dataset['train'])} pairs\")\n",
    "print(f\"Val:   {len(raw_dataset['validation'])} pairs\")\n",
    "print(f\"Test:  {len(raw_dataset['test'])} pairs\")\n",
    "print(f\"\\nExample: {raw_dataset['train'][0]}\")\n",
    "\n",
    "# 2. Train a shared BPE tokenizer (paper Section 5.1: shared source-target vocabulary)\n",
    "print(\"\\nTraining shared BPE tokenizer...\")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=config.vocab_size,\n",
    "    special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"],\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Combine DE + EN training text for shared vocabulary\n",
    "def training_corpus():\n",
    "    for example in raw_dataset[\"train\"]:\n",
    "        yield example[\"translation\"][\"de\"]\n",
    "        yield example[\"translation\"][\"en\"]\n",
    "\n",
    "tokenizer.train_from_iterator(training_corpus(), trainer=trainer)\n",
    "\n",
    "# Set up post-processing to add <bos> and <eos>\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> <bos> $B:1 <eos>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", config.bos_idx),\n",
    "        (\"<eos>\", config.eos_idx),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Update config with actual special token IDs\n",
    "config.pad_idx = tokenizer.token_to_id(\"<pad>\")\n",
    "config.bos_idx = tokenizer.token_to_id(\"<bos>\")\n",
    "config.eos_idx = tokenizer.token_to_id(\"<eos>\")\n",
    "config.unk_idx = tokenizer.token_to_id(\"<unk>\")\n",
    "config.vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "tokenizer.enable_padding(pad_id=config.pad_idx, pad_token=\"<pad>\")\n",
    "tokenizer.enable_truncation(max_length=config.max_seq_len)\n",
    "\n",
    "print(f\"Vocab size: {config.vocab_size}\")\n",
    "print(f\"Special tokens — PAD: {config.pad_idx}, BOS: {config.bos_idx}, EOS: {config.eos_idx}, UNK: {config.unk_idx}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_enc = tokenizer.encode(\"Hello, this is a test.\")\n",
    "print(f\"\\nTokenizer test:\")\n",
    "print(f\"  Input: 'Hello, this is a test.'\")\n",
    "print(f\"  Tokens: {test_enc.tokens}\")\n",
    "print(f\"  IDs: {test_enc.ids}\")\n",
    "print(f\"  Decoded: '{tokenizer.decode(test_enc.ids)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2b: Dataset & DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"IWSLT14 DE-EN translation dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, split_data, tokenizer, max_len):\n",
    "        self.pairs = []\n",
    "        for example in tqdm(split_data, desc=f\"Tokenizing\"):\n",
    "            src_text = example[\"translation\"][\"de\"]\n",
    "            tgt_text = example[\"translation\"][\"en\"]\n",
    "            src_enc = tokenizer.encode(src_text)\n",
    "            tgt_enc = tokenizer.encode(tgt_text)\n",
    "            self.pairs.append((\n",
    "                torch.tensor(src_enc.ids, dtype=torch.long),\n",
    "                torch.tensor(tgt_enc.ids, dtype=torch.long),\n",
    "            ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences to the longest in the batch.\"\"\"\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=config.pad_idx)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_seqs, batch_first=True, padding_value=config.pad_idx)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = TranslationDataset(raw_dataset[\"train\"], tokenizer, config.max_seq_len)\n",
    "val_dataset = TranslationDataset(raw_dataset[\"validation\"], tokenizer, config.max_seq_len)\n",
    "test_dataset = TranslationDataset(raw_dataset[\"test\"], tokenizer, config.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size, shuffle=True,\n",
    "    collate_fn=collate_fn, num_workers=2, pin_memory=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config.batch_size, shuffle=False,\n",
    "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Quick check\n",
    "src_batch, tgt_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes — src: {src_batch.shape}, tgt: {tgt_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Positional Encoding (Section 3.5)\n",
    "#\n",
    "# PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Quick visualization\n",
    "pe_module = PositionalEncoding(config.d_model)\n",
    "pe_values = pe_module.pe[0, :128, :].numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "cax = ax.imshow(pe_values.T, aspect='auto', cmap='RdBu')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Dimension')\n",
    "ax.set_title('Sinusoidal Positional Encoding')\n",
    "fig.colorbar(cax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Multi-Head Attention (Section 3.2)\n",
    "#\n",
    "# Scaled Dot-Product Attention:\n",
    "#   Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "#\n",
    "# Multi-Head Attention:\n",
    "#   MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W_O\n",
    "#   where head_i = Attention(Q W_Qi, K W_Ki, V W_Vi)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections (implemented as single large matrices)\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # Output projection\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q, k, v: (batch, n_heads, seq_len, d_k)\n",
    "        mask: broadcastable to (batch, n_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)  # Attention dropout (Section 5.4)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Linear projections and reshape to (batch, n_heads, seq_len, d_k)\n",
    "        q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Scaled dot-product attention\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # 3. Concatenate heads and apply output projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.w_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention ready.\")\n",
    "print(f\"  d_model={config.d_model}, n_heads={config.n_heads}, d_k={config.d_model // config.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Position-wise Feed-Forward Network (Section 3.3)\n",
    "#\n",
    "# FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "# Inner dimension d_ff, with ReLU activation.\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "print(\"PositionwiseFeedForward ready.\")\n",
    "print(f\"  d_model={config.d_model}, d_ff={config.d_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Encoder Layer & Encoder Stack (Section 3.1)\n",
    "#\n",
    "# Each encoder layer has two sub-layers:\n",
    "#   1. Multi-Head Self-Attention\n",
    "#   2. Position-wise FFN\n",
    "# Each sub-layer: LayerNorm(x + Sublayer(x))  [post-norm, as in the paper]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        # Sub-layer 1: Self-Attention with post-norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, src_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # LayerNorm(x + Dropout(Sublayer(x)))\n",
    "        \n",
    "        # Sub-layer 2: FFN with post-norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len, pad_idx):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        # Scale embeddings by sqrt(d_model) (Section 3.4)\n",
    "        x = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Encoder ready.\")\n",
    "print(f\"  {config.n_layers} layers, d_model={config.d_model}, n_heads={config.n_heads}, d_ff={config.d_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Decoder Layer & Decoder Stack (Section 3.1)\n",
    "#\n",
    "# Each decoder layer has three sub-layers:\n",
    "#   1. Masked Multi-Head Self-Attention (causal mask)\n",
    "#   2. Multi-Head Encoder-Decoder Attention (Q from decoder, K/V from encoder)\n",
    "#   3. Position-wise FFN\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # Sub-layer 1: Masked Self-Attention (post-norm)\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Sub-layer 2: Encoder-Decoder Attention (post-norm)\n",
    "        # Q from decoder, K and V from encoder output\n",
    "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # Sub-layer 3: FFN (post-norm)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len, pad_idx):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
    "        # Scale embeddings by sqrt(d_model) (Section 3.4)\n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Decoder ready.\")\n",
    "print(f\"  {config.n_layers} layers, d_model={config.d_model}, n_heads={config.n_heads}, d_ff={config.d_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Full Transformer (Section 3)\n",
    "#\n",
    "# Combines Encoder + Decoder + final linear projection.\n",
    "# Weight tying (Section 3.4): encoder embedding = decoder embedding = pre-softmax linear.\n",
    "\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    \"\"\"Create mask to hide padding tokens.\n",
    "    Returns: (batch, 1, 1, seq_len) — broadcastable over heads and query positions.\n",
    "    \"\"\"\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # 1 where not padding\n",
    "\n",
    "\n",
    "def create_causal_mask(size, device):\n",
    "    \"\"\"Create causal (look-ahead) mask for decoder self-attention.\n",
    "    Upper-triangular mask: position i can only attend to positions <= i.\n",
    "    Returns: (1, 1, size, size)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(size, size, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask  # 1 in lower triangle, 0 in upper triangle\n",
    "\n",
    "\n",
    "def create_tgt_mask(tgt, pad_idx):\n",
    "    \"\"\"Combined padding + causal mask for decoder.\"\"\"\n",
    "    tgt_pad_mask = create_padding_mask(tgt, pad_idx)  # (batch, 1, 1, tgt_len)\n",
    "    tgt_causal_mask = create_causal_mask(tgt.size(1), tgt.device)  # (1, 1, tgt_len, tgt_len)\n",
    "    return tgt_pad_mask & tgt_causal_mask  # broadcast: (batch, 1, tgt_len, tgt_len)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            config.vocab_size, config.d_model, config.n_layers,\n",
    "            config.n_heads, config.d_ff, config.dropout,\n",
    "            config.max_seq_len, config.pad_idx\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            config.vocab_size, config.d_model, config.n_layers,\n",
    "            config.n_heads, config.d_ff, config.dropout,\n",
    "            config.max_seq_len, config.pad_idx\n",
    "        )\n",
    "        \n",
    "        # Pre-softmax linear layer\n",
    "        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Three-way weight tying (Section 3.4):\n",
    "        # Encoder embedding = Decoder embedding = Output projection\n",
    "        self.decoder.embedding.weight = self.encoder.embedding.weight\n",
    "        self.output_projection.weight = self.encoder.embedding.weight\n",
    "        \n",
    "        # Initialize parameters with Xavier uniform (as is standard)\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n",
    "        return self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = create_padding_mask(src, self.config.pad_idx)\n",
    "        tgt_mask = create_tgt_mask(tgt, self.config.pad_idx)\n",
    "        \n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "        logits = self.output_projection(dec_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate and count parameters\n",
    "model = Transformer(config).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Transformer model created.\")\n",
    "print(f\"  Total parameters:     {n_params:,}\")\n",
    "print(f\"  Trainable parameters: {n_trainable:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Encoder: {config.n_layers} layers\")\n",
    "print(f\"  Decoder: {config.n_layers} layers\")\n",
    "print(f\"  d_model={config.d_model}, n_heads={config.n_heads}, d_ff={config.d_ff}\")\n",
    "print(f\"  Weight tying: encoder emb = decoder emb = output projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Label-Smoothed Cross-Entropy Loss (Section 5.4)\n",
    "#\n",
    "# True label gets probability (1 - eps), remaining eps distributed\n",
    "# uniformly across all other classes. Ignores padding index.\n",
    "# Paper: \"hurts perplexity but improves accuracy and BLEU.\"\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_idx, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_idx = pad_idx\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "    \n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: (batch * seq_len, vocab_size)\n",
    "        target: (batch * seq_len,)\n",
    "        \"\"\"\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Create smoothed target distribution\n",
    "        # Fill with uniform smoothing value\n",
    "        smooth_targets = torch.full_like(log_probs, self.smoothing / (self.vocab_size - 2))  # -2 for pad and true label\n",
    "        # Set the true label's probability\n",
    "        smooth_targets.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        # Zero out padding\n",
    "        smooth_targets[:, self.pad_idx] = 0\n",
    "        \n",
    "        # Mask out padding positions entirely\n",
    "        pad_mask = target == self.pad_idx\n",
    "        smooth_targets[pad_mask] = 0\n",
    "        \n",
    "        # KL divergence: sum over vocab, mean over non-pad tokens\n",
    "        loss = -(smooth_targets * log_probs).sum(dim=-1)\n",
    "        loss = loss[~pad_mask].mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "criterion = LabelSmoothingLoss(config.vocab_size, config.pad_idx, config.label_smoothing)\n",
    "print(f\"Label Smoothing Loss ready (eps={config.label_smoothing})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Optimizer & LR Schedule (Section 5.3)\n",
    "#\n",
    "# lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
    "# Linear warmup for warmup_steps, then decay proportional to 1/sqrt(step).\n",
    "\n",
    "def get_lr_lambda(d_model, warmup_steps):\n",
    "    def lr_lambda(step):\n",
    "        step = max(step, 1)  # avoid division by zero\n",
    "        return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
    "    return lr_lambda\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1.0,  # actual LR controlled by scheduler\n",
    "    betas=(config.adam_beta1, config.adam_beta2),\n",
    "    eps=config.adam_eps\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=get_lr_lambda(config.d_model, config.warmup_steps)\n",
    ")\n",
    "\n",
    "# Visualize the LR schedule\n",
    "steps = list(range(1, 60001))\n",
    "lr_fn = get_lr_lambda(config.d_model, config.warmup_steps)\n",
    "lrs = [lr_fn(s) for s in steps]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.plot(steps, lrs)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title(f'Transformer LR Schedule (warmup={config.warmup_steps}, d_model={config.d_model})')\n",
    "ax.axvline(x=config.warmup_steps, color='r', linestyle='--', alpha=0.5, label=f'warmup={config.warmup_steps}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Peak LR at step {config.warmup_steps}: {lr_fn(config.warmup_steps):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training Loop\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "    for i, (src, tgt) in enumerate(pbar):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n",
    "        tgt_input = tgt[:, :-1]   # everything except last token\n",
    "        tgt_output = tgt[:, 1:]   # everything except first token (<bos>)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss: (batch * seq_len, vocab_size)\n",
    "        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "        tgt_flat = tgt_output.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(logits_flat, tgt_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track stats\n",
    "        n_tokens = (tgt_output != config.pad_idx).sum().item()\n",
    "        total_loss += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n",
    "                'tok/s': f'{total_tokens / elapsed:.0f}'\n",
    "            })\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "        logits_flat = logits.contiguous().view(-1, logits.size(-1))\n",
    "        tgt_flat = tgt_output.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(logits_flat, tgt_flat)\n",
    "        \n",
    "        n_tokens = (tgt_output != config.pad_idx).sum().item()\n",
    "        total_loss += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "print(\"Training functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Greedy Decoding\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, src, src_mask, max_len, bos_idx, eos_idx, device):\n",
    "    \"\"\"\n",
    "    Autoregressively generate target tokens using greedy (argmax) decoding.\n",
    "    \n",
    "    Args:\n",
    "        src: (1, src_len) source token IDs\n",
    "        src_mask: (1, 1, 1, src_len) padding mask\n",
    "        max_len: maximum generation length\n",
    "    Returns:\n",
    "        List of generated token IDs (excluding <bos>)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source once\n",
    "    enc_output = model.encode(src, src_mask)\n",
    "    \n",
    "    # Start with <bos> token\n",
    "    ys = torch.tensor([[bos_idx]], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = create_causal_mask(ys.size(1), device)\n",
    "        dec_output = model.decode(ys, enc_output, src_mask, tgt_mask)\n",
    "        logits = model.output_projection(dec_output[:, -1, :])  # last position\n",
    "        next_token = logits.argmax(dim=-1, keepdim=True)  # (1, 1)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return ys[0, 1:].tolist()  # exclude <bos>\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, tokenizer, config, device):\n",
    "    \"\"\"Translate a single German sentence to English.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize source\n",
    "    src_enc = tokenizer.encode(sentence)\n",
    "    src = torch.tensor([src_enc.ids], dtype=torch.long, device=device)\n",
    "    src_mask = create_padding_mask(src, config.pad_idx)\n",
    "    \n",
    "    # Decode\n",
    "    output_ids = greedy_decode(\n",
    "        model, src, src_mask, config.max_seq_len,\n",
    "        config.bos_idx, config.eos_idx, device\n",
    "    )\n",
    "    \n",
    "    # Remove <eos> if present\n",
    "    if output_ids and output_ids[-1] == config.eos_idx:\n",
    "        output_ids = output_ids[:-1]\n",
    "    \n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "print(\"Greedy decoding ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Beam Search Decoding\n",
    "#\n",
    "# Paper: beam size 4, length penalty alpha=0.6\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, src_mask, max_len, bos_idx, eos_idx, device,\n",
    "                       beam_size=4, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Beam search decoding with length normalization.\n",
    "    \n",
    "    Length normalization: score / (length^alpha)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    enc_output = model.encode(src, src_mask)  # (1, src_len, d_model)\n",
    "    \n",
    "    # Each beam: (score, [token_ids])\n",
    "    beams = [(0.0, [bos_idx])]\n",
    "    completed = []\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        all_candidates = []\n",
    "        \n",
    "        for score, seq in beams:\n",
    "            if seq[-1] == eos_idx:\n",
    "                # Length-normalized score\n",
    "                norm_score = score / (len(seq) ** alpha)\n",
    "                completed.append((norm_score, seq))\n",
    "                continue\n",
    "            \n",
    "            ys = torch.tensor([seq], dtype=torch.long, device=device)\n",
    "            tgt_mask = create_causal_mask(ys.size(1), device)\n",
    "            dec_output = model.decode(ys, enc_output, src_mask, tgt_mask)\n",
    "            logits = model.output_projection(dec_output[:, -1, :])\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            topk_log_probs, topk_ids = log_probs.topk(beam_size, dim=-1)\n",
    "            \n",
    "            for k in range(beam_size):\n",
    "                new_score = score + topk_log_probs[0, k].item()\n",
    "                new_seq = seq + [topk_ids[0, k].item()]\n",
    "                all_candidates.append((new_score, new_seq))\n",
    "        \n",
    "        if not all_candidates:\n",
    "            break\n",
    "        \n",
    "        # Keep top beam_size candidates\n",
    "        all_candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = all_candidates[:beam_size]\n",
    "    \n",
    "    # Add remaining beams to completed\n",
    "    for score, seq in beams:\n",
    "        norm_score = score / (len(seq) ** alpha)\n",
    "        completed.append((norm_score, seq))\n",
    "    \n",
    "    # Return best completed sequence\n",
    "    completed.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_seq = completed[0][1]\n",
    "    \n",
    "    # Remove <bos> and <eos>\n",
    "    result = best_seq[1:]  # remove <bos>\n",
    "    if result and result[-1] == eos_idx:\n",
    "        result = result[:-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(f\"Beam search decoding ready (beam_size=4, alpha=0.6).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: BLEU Evaluation\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "def compute_bleu(model, test_loader, tokenizer, config, device, use_beam=False):\n",
    "    \"\"\"Compute corpus-level BLEU on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for src_batch, tgt_batch in tqdm(test_loader, desc=\"Translating test set\"):\n",
    "        for i in range(src_batch.size(0)):\n",
    "            # Get source (remove padding)\n",
    "            src = src_batch[i].unsqueeze(0).to(device)\n",
    "            src_mask = create_padding_mask(src, config.pad_idx)\n",
    "            \n",
    "            # Decode\n",
    "            if use_beam:\n",
    "                output_ids = beam_search_decode(\n",
    "                    model, src, src_mask, config.max_seq_len,\n",
    "                    config.bos_idx, config.eos_idx, device\n",
    "                )\n",
    "            else:\n",
    "                output_ids = greedy_decode(\n",
    "                    model, src, src_mask, config.max_seq_len,\n",
    "                    config.bos_idx, config.eos_idx, device\n",
    "                )\n",
    "                # Remove <eos> if present\n",
    "                if output_ids and output_ids[-1] == config.eos_idx:\n",
    "                    output_ids = output_ids[:-1]\n",
    "            \n",
    "            # Decode to text\n",
    "            pred_text = tokenizer.decode(output_ids)\n",
    "            predictions.append(pred_text)\n",
    "            \n",
    "            # Reference: decode target tokens (remove <bos>, <eos>, <pad>)\n",
    "            ref_ids = tgt_batch[i].tolist()\n",
    "            ref_ids = [t for t in ref_ids if t not in (config.pad_idx, config.bos_idx, config.eos_idx)]\n",
    "            ref_text = tokenizer.decode(ref_ids)\n",
    "            references.append(ref_text)\n",
    "    \n",
    "    # Compute BLEU using sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    return bleu, predictions, references\n",
    "\n",
    "print(\"BLEU evaluation ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Run Training\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(f\"Training for {config.n_epochs} epochs...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for epoch in range(1, config.n_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_transformer.pt')\n",
    "        marker = ' *'\n",
    "    else:\n",
    "        marker = ''\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Time: {epoch_time:.1f}s | LR: {scheduler.get_last_lr()[0]:.2e}{marker}\")\n",
    "    \n",
    "    # Show a sample translation every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        sample_de = raw_dataset['validation'][0]['translation']['de']\n",
    "        sample_en = raw_dataset['validation'][0]['translation']['en']\n",
    "        pred_en = translate_sentence(model, sample_de, tokenizer, config, device)\n",
    "        print(f\"  Sample — DE: {sample_de}\")\n",
    "        print(f\"           EN: {sample_en}\")\n",
    "        print(f\"         Pred: {pred_en}\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plot loss curves\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "ax.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training & Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Run Evaluation & Show Results\n",
    "\n",
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load('best_transformer.pt', map_location=device))\n",
    "print(\"Loaded best model checkpoint.\\n\")\n",
    "\n",
    "# Compute BLEU on test set (greedy decoding)\n",
    "print(\"Evaluating with greedy decoding...\")\n",
    "bleu_greedy, preds_greedy, refs = compute_bleu(model, test_loader, tokenizer, config, device, use_beam=False)\n",
    "print(f\"\\nGreedy BLEU: {bleu_greedy.score:.2f}\")\n",
    "print(bleu_greedy)\n",
    "\n",
    "# Compute BLEU with beam search\n",
    "print(\"\\nEvaluating with beam search (beam=4, alpha=0.6)...\")\n",
    "bleu_beam, preds_beam, _ = compute_bleu(model, test_loader, tokenizer, config, device, use_beam=True)\n",
    "print(f\"\\nBeam Search BLEU: {bleu_beam.score:.2f}\")\n",
    "print(bleu_beam)\n",
    "\n",
    "# Show sample translations\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Sample Translations (from test set)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "n_samples = 10\n",
    "indices = random.sample(range(len(refs)), n_samples)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    src_text = raw_dataset['test'][idx]['translation']['de']\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"  Source (DE):     {src_text}\")\n",
    "    print(f\"  Reference (EN):  {refs[idx]}\")\n",
    "    print(f\"  Greedy (EN):     {preds_greedy[idx]}\")\n",
    "    print(f\"  Beam (EN):       {preds_beam[idx]}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"  Greedy BLEU:      {bleu_greedy.score:.2f}\")\n",
    "print(f\"  Beam Search BLEU: {bleu_beam.score:.2f}\")\n",
    "print(f\"  Model params:     {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Config: N={config.n_layers}, d_model={config.d_model}, d_ff={config.d_ff}, h={config.n_heads}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
